{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 政府网站公开数据爬取"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 爬取漳州历史天气数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "header = {\n",
    "    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36',\n",
    "}\n",
    "res_list = []\n",
    "for year in range(2020,2022):\n",
    "    for month in range(1,13):\n",
    "        url = 'https://lishi.tianqi.com/zhangzhou/{}{:0>2d}.html'.format(year,month)\n",
    "        # print(url)\n",
    "        res = requests.get(url=url,headers=header)\n",
    "        res_list.append(res)\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "# 2020年每天的天气情况\n",
    "day = 1\n",
    "csv_str = \"\"\n",
    "for month in range(1,13):\n",
    "    dom = etree.HTML(res_list[month].text)\n",
    "    values = dom.xpath('/html/body/div[7]/div[1]/div[4]/ul/li')\n",
    "    for one in values:\n",
    "        high_temp = int(one.xpath('./div[2]/text()')[0][0:-1])\n",
    "        low_temp = int(one.xpath('./div[3]/text()')[0][0:-1])\n",
    "        weather = one.xpath('./div[4]/text()')[0]\n",
    "\n",
    "        # print(day,high_temp,low_temp,weather)\n",
    "        line = \"{},{},{},{}\\n\".format(day,high_temp,low_temp,weather)\n",
    "        csv_str = csv_str + line\n",
    "        day += 1\n",
    "\n",
    "head_str = \"day,high,low,weather\\n\"\n",
    "csv_str = head_str + csv_str\n",
    "# print(csv_str)\n",
    "with open('weather.csv','w',encoding='utf-8') as f:\n",
    "    f.write(csv_str)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 福建\n",
    "\n",
    "爬取福建省生态环保厅的数据\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  年             周            pH            DO           COD  \\\n",
      "count  17578.000000  17578.000000  16371.000000  16357.000000  16225.000000   \n",
      "mean    2015.667767     26.162760      7.028330      7.763802      2.360920   \n",
      "std        4.257231     14.900942      0.481336      8.138108      2.813928   \n",
      "min     2004.000000      1.000000      4.930000      1.390000      0.030000   \n",
      "25%     2013.000000     13.000000      6.720000      6.760000      1.700000   \n",
      "50%     2017.000000     26.000000      6.970000      7.630000      2.200000   \n",
      "75%     2019.000000     39.000000      7.260000      8.620000      2.780000   \n",
      "max     2021.000000     53.000000     10.000000    737.000000    303.000000   \n",
      "\n",
      "                 TP            氨氮           总氮  \n",
      "count  14904.000000  13138.000000  7134.000000  \n",
      "mean       0.078706      0.250965     1.875494  \n",
      "std        0.078206      0.295300     1.786630  \n",
      "min        0.000000      0.000000     0.051000  \n",
      "25%        0.035000      0.080000     0.840000  \n",
      "50%        0.062000      0.170000     1.300000  \n",
      "75%        0.097000      0.320000     2.420000  \n",
      "max        3.300000     10.000000    26.840000  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "header = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36'}\n",
    "url = 'http://sthjt.fujian.gov.cn/was5/web/search'\n",
    "params = {\n",
    "    'channelid':280067,\n",
    "    'sortfield':'-s4',\n",
    "    'classsql':'(dockind=10)',\n",
    "    'r':'0.3624881561901028',\n",
    "    'prepage':100,\n",
    "    'page':1\n",
    "}\n",
    "res_list = []\n",
    "for i in range(1,177):\n",
    "    params['page'] = i\n",
    "    res = requests.get(url=url,params=params,headers=header)\n",
    "    res_list.append(res)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print('done')\n",
    "\n",
    "\n",
    "import json\n",
    "docs_list = []\n",
    "for one in res_list:\n",
    "    doc = json.loads(one.text.replace('\\r','').replace('\\n',''))\n",
    "    docs_list = docs_list + doc['docs']\n",
    "print('done 1')\n",
    "\n",
    "\n",
    "csv_str = \"\"\n",
    "for i in range(len(docs_list)):\n",
    "    one = docs_list[i]\n",
    "    try:\n",
    "        line = \"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(one['s1'] ,one['s2'],one['s3'],\n",
    "                                           one['s4'],one['s5'],one['s6'],\n",
    "                                           one['s7'],one['f1'],one['f2'],\n",
    "                                           one['f3'],one['f4'],one['f5'],one['f6'],)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        # print(i)\n",
    "        continue\n",
    "    csv_str = csv_str + line\n",
    "    # if i  == 2:\n",
    "    #     break\n",
    "\n",
    "# print(csv_str)\n",
    "print('done 2')\n",
    "\n",
    "\n",
    "head_str = '水系,点位名称,断面名称,年,周,起始时间,结束时间,pH,DO,COD,TP,氨氮,总氮\\n'\n",
    "csv_str = head_str + csv_str\n",
    "with open('water.csv','w',encoding='utf-8') as f:\n",
    "    f.write(csv_str)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fujian_df = pd.read_csv('data/network/gov/fujian/fujian.csv')\n",
    "\n",
    "print(fujian_df.describe())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004年记录总数:126\n",
      "2005年记录总数:243\n",
      "2006年记录总数:418\n",
      "2007年记录总数:421\n",
      "2008年记录总数:406\n",
      "2009年记录总数:604\n",
      "2010年记录总数:654\n",
      "2011年记录总数:652\n",
      "2012年记录总数:390\n",
      "2013年记录总数:729\n",
      "2014年记录总数:676\n",
      "2015年记录总数:676\n",
      "2016年记录总数:689\n",
      "2017年记录总数:3520\n",
      "2018年记录总数:2820\n",
      "2019年记录总数:1716\n",
      "2020年记录总数:1749\n",
      "2021年记录总数:1089\n"
     ]
    }
   ],
   "source": [
    "fujian_df['年'] = fujian_df['年'].astype(int)\n",
    "\n",
    "for year in range(2004,2022):\n",
    "    year_df = fujian_df[(fujian_df['年'] >= year) & (fujian_df['年'] < year + 1) ]\n",
    "    print(\"{}年记录总数:{}\".format(year,len(year_df)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 统计站点及其记录数\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "记录总数:3520\n",
      "站点总数：58\n",
      "姑田溪(龙岩-三明交界断面),55\n",
      "尤溪(三明－南平交界断面),55\n",
      "干流(对照),55\n",
      "翠江河(宁化-清流交界断面),55\n",
      "干流(周宁-福安交界断面),55\n",
      "饮用水水源地,330\n",
      "水库、湖泊,110\n",
      "富屯溪(三明-南平交界断面),55\n",
      "黄潭河(上杭-永定交界断面),55\n",
      "西溪(龙岩-漳州交界断面),55\n",
      "干流(上杭湖洋乡涧头村),55\n",
      "干流(寿宁-福安交界断面),55\n",
      "九龙溪(清流-永安交界断面),55\n",
      "沙溪(三明-南平交界断面),55\n",
      "富屯溪(邵武-顺昌交界断面),55\n",
      "北溪(控制断面),55\n",
      "西溪(安溪-南安交界断面),55\n",
      "濉溪(建宁-泰宁交界断面),55\n",
      "黄潭河(新罗-上杭交界断面),55\n",
      "鱼塘溪(明溪-三元交界断面),55\n",
      "建溪(武夷山-建阳交界断面),55\n",
      "干流(宁德-福州交界断面),55\n",
      "干流(仙游-城厢交界断面),55\n",
      "北团溪(龙岩-三明交界断面),55\n",
      "北溪 (龙岩－漳州交界断面),55\n",
      "干流(宁德－福州交界断面),55\n",
      "干流(南安－丰泽交界断面),55\n",
      "北溪(新罗-漳平交界断面),55\n",
      "北溪 (华安－芗城交界断面),55\n",
      "北溪(厦门－漳州交界断面),55\n",
      "桃溪(永春-南安交界断面),55\n",
      "干流(罗源－连江交界断面),55\n",
      "干流(屏南-蕉城交界断面),55\n",
      "沙溪(永安-三元交界断面),55\n",
      "沙溪(梅列-沙县交界断面),55\n",
      "金溪(泰宁-将乐交界断面),55\n",
      "富屯溪(光泽-邵武交界断面),55\n",
      "富屯溪(顺昌-延平交界断面),55\n",
      "建溪(建阳-建瓯交界断面),55\n",
      "省界(浙－闽),55\n",
      "建溪(政和－建瓯交界断面),55\n",
      "均溪(大田－尤溪交界断面),55\n",
      "干流(南平-宁德交界断面),55\n",
      "建溪(浦城－建阳交界断面),55\n",
      "建溪(松溪－政和交界断面),55\n",
      "建溪(建瓯－延平交界断面),55\n",
      "干流(闽侯-福州交界断面),55\n",
      "大樟溪(泉州-福州交界断面),55\n",
      "干流(闽江入海口),55\n",
      "北溪(长泰-龙文交界断面),55\n",
      "西溪(平和-南靖交界断面),55\n",
      "西溪(南靖-芗城交界断面),55\n",
      "干流(长汀-上杭交界断面),55\n",
      "干流(上杭-永定交界断面),55\n",
      "文川河(龙岩-三明交界断面),55\n",
      "干流(连江-马尾交界断面),55\n",
      "大樟溪(永泰-闽侯交界断面),55\n",
      "干流(闽清-闽侯交界断面),55\n"
     ]
    }
   ],
   "source": [
    "year_df = fujian_df[(fujian_df['年'] >= 2017) & (fujian_df['年'] < 2018) ]\n",
    "print(\"记录总数:{}\".format(len(year_df)))\n",
    "site_list = year_df['断面名称'].unique()\n",
    "print(\"站点总数：{}\".format(len(site_list)))\n",
    "for site in site_list:\n",
    "    one = year_df[year_df['断面名称'] ==  site]\n",
    "    print(\"{},{}\".format(site,len(one)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 山东\n",
    "\n",
    "爬取山东省生态环保厅的数据\n",
    "\n",
    "http://sthj.shandong.gov.cn/\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "站点总数：1691\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://fb.sdem.org.cn:8801/wryfb/ajax/map.ashx'\n",
    "param = {\n",
    "    'Method':'SelectSubList',\n",
    "    'stcode':'0',\n",
    "    'type':'WasteWaterGis',\n",
    "    'isall':'0'\n",
    "}\n",
    "header = {\n",
    "    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36',\n",
    "    # 'Referer':'http://fb.sdem.org.cn:8801/wryfb/WebGis/WasteWaterGis/HistoryData_New.aspx?SubId=29941%20&SubName=%u79D1%u6CD3%u79D1%u6280%u96C6%u56E2%u6709%u9650%u516C%u53F8',\n",
    "    # 'X-Requested-With':'XMLHttpRequest',\n",
    "    # 'Origin':'http://fb.sdem.org.cn:8801',\n",
    "    # 'Pragma':'no-cache',\n",
    "    # 'Cookie':'ASP.NET_SessionId=y5zc211bfydoziryjxzimesx; ASP.NET_SessionId_NS_Sig=oenCV6md0Dtq6Bby',\n",
    "    # 'Content-Type':'application/x-www-form-urlencoded; charset=UTF-8'\n",
    "}\n",
    "res = requests.post(url=url, data=param, headers=header)\n",
    "site_list = json.loads(res.text)['items']\n",
    "print(\"站点总数：{}\".format(len(site_list)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 获取站点列表\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取2010年\n",
      "爬取2011年\n",
      "爬取2012年\n",
      "爬取2013年\n",
      "爬取2014年\n",
      "爬取2015年\n",
      "爬取2016年\n",
      "爬取2017年\n",
      "爬取2018年\n",
      "爬取2019年\n",
      "爬取2020年\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "url = 'http://fb.sdem.org.cn:8801/wryfb/ajax/WasteWaterGis/WasteWaterHandler.ashx'\n",
    "param = {\n",
    "    'Method':'GetHisChart_New',\n",
    "    'strID':'2326',\n",
    "    'strTime':'2021-07-01'\n",
    "}\n",
    "header = {\n",
    "    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36',\n",
    "}\n",
    "month_data_list = []\n",
    "for year in range(2010,2021):\n",
    "    print(\"爬取{}年\".format(year))\n",
    "    for month in range(1,13):\n",
    "        str_time = '{}-{}-01'.format(year,month)\n",
    "        param['strTime'] = str_time\n",
    "\n",
    "        month_res = requests.post(url=url, data=param, headers=header)\n",
    "        month_data = json.loads(month_res.text)\n",
    "        month_data_list.append(month_data)\n",
    "\n",
    "        time.sleep(0.05)\n",
    "        # break\n",
    "    # break\n",
    "\n",
    "# print(res.text)\n",
    "# doc = json.loads(res.text)\n",
    "# print(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 获取站点数据\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 中国环境监测总站  水质自动检测周报\n",
    "\n",
    "http://www.cnemc.cn/sssj/szzdjczb/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "page_list = []\n",
    "for i in range(25):\n",
    "\n",
    "    if i == 0:\n",
    "        page = requests.get(url='http://www.cnemc.cn/sssj/szzdjczb/index.shtml')\n",
    "    else:\n",
    "        url = 'http://www.cnemc.cn/sssj/szzdjczb/index_{}.shtml'.format(i)\n",
    "        page = requests.get(url=url)\n",
    "    time.sleep(0.05)\n",
    "    page_list.append(page)\n",
    "print('done')\n",
    "# print(page.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 获取25页\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./201901/P020190130355018826755.doc可以直接下载\n",
      "./201901/P020190130354704630808.doc可以直接下载\n",
      "./201901/P020190130354332800637.doc可以直接下载\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201807/t20180719_660245.shtml没有找到下载链接\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201510/t20151016_660152.shtml没有找到下载链接\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201510/t20151008_660151.shtml没有找到下载链接\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201509/t20150924_660150.shtml没有找到下载链接\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201509/t20150920_660149.shtml没有找到下载链接\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201509/t20150920_660148.shtml没有找到下载链接\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201509/t20150907_660147.shtml没有找到下载链接\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201508/t20150825_660146.shtml没有找到下载链接\n",
      "http://www.cnemc.cn/sssj/szzdjczb/201508/t20150823_660145.shtml没有找到下载链接\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url_base = 'http://www.cnemc.cn/sssj/szzdjczb'\n",
    "\n",
    "for page in page_list:\n",
    "    dom = etree.HTML(page.text)\n",
    "    for i in range(1,21):\n",
    "        xpath_str = '//*[@id=\"contentPageData\"]/li[{}]/a/@href'.format(i)\n",
    "        line = dom.xpath(xpath_str)[0]\n",
    "        if line.split('.')[-1] == 'shtml':\n",
    "            url = url_base + line[1:]\n",
    "            page =  requests.get(url=url)\n",
    "\n",
    "            # 获取下载页\n",
    "            doc_page = etree.HTML(page.text)\n",
    "            # 获取所有a标签的链接\n",
    "            all_link = doc_page.xpath('//a/@href')\n",
    "            found = False\n",
    "            for link in all_link:\n",
    "                last_str = link.split('.')[-1]\n",
    "                if (last_str == 'doc' or last_str == 'pdf') and link[:2] == './':\n",
    "                    found = True\n",
    "                    # 获取下载链接\n",
    "                    download_link = \"/\".join(url.split(\"/\")[:-1]) + link[1:]\n",
    "                    doc_file = requests.get(url=download_link)\n",
    "\n",
    "                    # 下载保存\n",
    "                    dir = 'data/network/gov/cnemc/'\n",
    "                    filename = dir + link[2:]\n",
    "                    with open(filename,'wb') as f:\n",
    "                        f.write(doc_file.content)\n",
    "                    # print(download_link)\n",
    "            if not found:\n",
    "                print('{}没有找到下载链接'.format(url))\n",
    "\n",
    "            time.sleep(0.05)\n",
    "            # break\n",
    "        else:\n",
    "            print('{}可以直接下载'.format(line))\n",
    "    # break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 下载所有周报\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成！\n"
     ]
    }
   ],
   "source": [
    "import os #用于获取目标文件所在路径\n",
    "path=\"E:\\\\project\\\\mvp\\\\Graph-WaveNet\\\\data\\\\network\\\\gov\\\\cnemc\\\\\" # 文件夹绝对路径\n",
    "files=[]\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".doc\"): #排除文件夹内的其它干扰文件，只获取\".doc\"后缀的word文件\n",
    "        files.append(path+file)\n",
    "\n",
    "from win32com import client as wc #导入模块\n",
    "word = wc.Dispatch(\"Word.Application\") # 打开word应用程序\n",
    "for file in files:\n",
    "    doc = word.Documents.Open(file) #打开word文件\n",
    "    doc.SaveAs(\"{}x\".format(file), 12)#另存为后缀为\".docx\"的文件，其中参数12指docx文件\n",
    "    doc.Close() #关闭原来word文件\n",
    "word.Quit()\n",
    "print(\"完成！\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% doc 转为docx\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# import docx\n",
    "from docx import Document\n",
    "\n",
    "docFile = 'data/network/gov/cnemc/2018-51.docx'\n",
    "document = Document(docFile) #读入文件\n",
    "table = document.tables[0] #获取文件中的表格集\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 读取doc文档\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'1'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.cell(2,0).text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}